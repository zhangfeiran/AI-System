<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 模型压缩与加速 (Model Compression and Acceleration)

 

在深度学习中，模型压缩通常是指通过特定的方法使用更少的数据比特表示原有模型，类似于传统计算机科学中的数据压缩或视频编码。
学术界高精度的深度学习模型在落地部署到工业界应用的过程中，经常面临着低吞吐，高延迟和高功耗的挑战。
模型压缩可以删除模型中的冗余，进而减少对硬件的存储需求和计算需求，以达到加速模型推理或训练的目的。
适当的压缩通常可以保持模型的原有效果，但是在不同场景和任务中，不同模型能实现的压缩率也不尽相同。
近年来广泛使用的模型压缩方法主要包括：**数值量化（Data Quantization）**，**模型稀疏化（Model sparsification）**，**知识蒸馏（Knowledge Distillation）**，
**轻量化网络设计（Lightweight Network Design）**和**张量分解（Tensor Decomposition）**。
本章第1节将首先对这些压缩技术进行简要介绍。
其中模型稀疏化是应用最为广泛的一种模型压缩方法，可以直接减少模型中的参数量。
本章第2节将对基于稀疏化的模型压缩方法进行详细介绍。
经过压缩后的模型并一定适用于原有通用处理器，往往需要特定的加速库或者加速硬件的支持。
本章第3节将介绍不同模型压缩算法所适应的硬件加速方案。



本章包含以下内容：

- [11.1 模型压缩简介](11.1-模型压缩简介.md)
- [11.2 基于稀疏化的模型压缩](11.2-基于稀疏化的模型压缩.md)
- [11.3 模型压缩与硬件加速](11.3-模型压缩与硬件加速.md)

```toc
:maxdepth: 2
:numbered:

11.1-模型压缩简介
11.2-基于稀疏化的模型压缩
11.3-模型压缩与硬件加速
```
    
<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/microsoft/AI-System/blob/main/LICENSE)版权许可-->

# 10.2.1 分布式强化学习算法
- [10.2.1 分布式强化学习算法](#1021-分布式强化学习算法)
  - [基本概念](#基本概念)
  - [分布式强化学习算法的发展](#分布式强化学习算法的发展)
    - [DQN](#dqn)
    - [GORILA](#gorila)
    - [A3C](#a3c)
    - [ApeX](#apex)
    - [IMPALA](#impala)
    - [SEEDRL](#seedrl)
  - [小结与讨论](#小结与讨论)
  - [参考文献](#参考文献)

深度强化学习算法的发展与进步让许多行业受益。然而复杂的应用场景和大规模的工业应用对算法的要求也特别高，与此同时，深度强化学习由于需要探索试错的模式进行学习，因而强化学习需要探索大量的数据。模型的计算时间长、收敛速度慢是导致整个强化学习的迭代速度慢的重要原因，从而也抑制了强化学习在工业界的快速发展。传统的单机CPU、GPU等计算已经远远不能够满足大数据时代的要求，经典的分布式集群(多机)、GPU集群运算开始进入了深度强化学习领域。


|  游戏智能体  | CPU cores | GPU cores | TPU cores | 
|  :----    |     ----  |     ----  |     ----  |
| AlphaStar[<sup>[7]</sup>](#alphastar) | ~50，000  | N/A | ~3，000 |
|OpenAI Five[<sup>[8]</sup>](#openfive) | ~80,000 - 173,000 | 480 - 1,536 | N/A |
| 王者荣耀5v5[<sup>[9]</sup>](#moba) | 300,000 | ~200 | N/A |


从表中可以看到，在复杂游戏中训练一个高水平的智能体所需的计算资源是巨大的。尽管目前学术界有许多工作在想方设法地优化算法来提升样本利用率，但是整体上来讲，目前深度强化学习对于训练样本的需求量仍然是非常惊人的。所以，也有不少工作在致力于如何更高效地利用计算资源，设计更好的计算架构，从而在更短的时间内产生更多的样本，达到更好的训练效果。许多分布式强化学习（Distributed Reinforcement Learning）的算法和架构应运而生，分布式强化学习提到了极大的发展。

  <div align="center">
  <img src="./img/distributed_rl_development_update.png" ch="1000" />
  </div>
  <div align=center>图10.2.1 分布式强化学习发展路线</div>

  例如，在图10.2.1里展示了和DQN相关的分布式强化学习的发展路线，从2013的DQN到2019年的SEEDRL，许多研究人员基于前人的工作不断地向前改进和发展。在下面的章节里，我们首先会介绍一些和分布式强化学习相关的基本概念，然后我们会讨论这些算法是如何演化的。

## 基本概念

- **采样器**（Actor)：负责和环境交互采集数据，从学习器里拿到推理用的模型，将采样到地数据发给学习器或者重放缓冲区。采样器和环境交互时，根据当前状态给环境发送要执行的动作，环境返回下一个状态和单步的奖励。采样器将历史的数据（包括当前状态，动作，下一步状态等）收集起来，为学习器的训练样本提供储备。

- **学习器**（Learner）：主要功能是拿到训练样本来训练强化学习的模型（例如：策略网络或者价值网络），更新模型参数。

- **重放缓冲区**（Replay Buffer）: 用来缓存采样到的数据。用户可以定义特定的采样策略，通过采样策略生成训练样本给学习器。对于在线策略学习来说，重放缓冲区通常是一个先进先出的队列；而对于某些离线策略算法（例如，DQN）来说，重放缓冲区通常被设计为根据优先级采样的存储器。

- **行为策略**（Behavior Policy）：采样器和环境交互时采用的策略。通常存在于采样器中，区别于目标策略。

- **目标策略**（Target Policy）：根据行为策略产生的样本不断学习和优化的策略，即训练完成最终用来使用的策略。

- **在线策略**（On-policy）算法和**离线策略**（Off-policy）算法：在线策略算法就是指这类算法要求行为策略和目标策略得保持一致，而离线策略算法则不需要这个限制条件，目标策略可以根据任意行为策略产生的样本来学习和优化。

- **架构**和**交互方式**：架构指的是强化学习里不同模块之间连接关系，以及不同模块使用的硬件资源。交互的方式指的是模块之间数据流动的方式以及传输的数据内容。模块之间的交互方式包括同步或者异步等。

## 分布式强化学习算法的发展

我们根据时间轴来讲述分布式强化学习算法和架构的发展和变化。

### DQN

原始的**DQN**[<sup>[1]</sup>](#dqn)的架构非常简单，所有的模块都可以由单进程里实现。

如图10.2.2所示： 
首先，**采样器**在**环境**里交互并采集训练数据。采样器根据当前状态 $s$ 做出动作 $a$，环境收到动作以后返回下一个状态 $s^{'}$ 和 单步的奖励 $r$ 给采样器。
采样器将收集到的一系列的 $(s, a, s^{'})$ 数据放到**重放缓冲区**里。
**学习器**每隔一定的间隔，将数据从重放缓冲区里拿出并更新强化学习Q网络（Q Network）；而采样器也会用新更新得Q网络来进行新一轮数据的采样。

很显然，当学习器和采样器在一个进程里的时候，他们互相等待且不能并行，即：学习器要等待采样器收集新的训练数据，而采样器在开始下一轮收集数据之前要更新当前的行为策略。这降低了学习器和采样器的工作效率。即使将他们并行起来，由于学习器需要大量的数据去拟合，而单个采样器的效率太低，导致学习器大部分时间处于效率不高的状态。

  <div align="center">
  <img src="./img/DQN_arch.png" ch="500" width=70% />
  </div>
  <div align=center>图10.2.2 DQN的架构 </div>

### GORILA

**Gorila**[<sup>[2]</sup>](#gorila)是早期的将深度强化学习拓展到大规模并行场景的经典工作之一。
当时深度强化学习的SOTA还是DQN算法，因此该工作基于DQN提出了变体，拓展到大规模并行的场景。

在该架构中，采样器和学习器不必再互相等待。如图10.2.3所示，在Gorila的架构中，**学习器**可以是多个实例。并且每个学习器中的Q网络的参数梯度会发给**参数服务器**（Parameter Server）。**参数服务器**收到后以异步SGD的方式更新网络模型。这个模型以一定的频率同步到采样器中。同样的，在Gorila的架构里，采样器也可以是多个实例。**采样器**基于该模型产生动作在环境中采样，产生的经验轨迹发往**重放缓冲区**。重放缓冲区中的数据再被学习器采样拿去学习。另外，每过N步学习器还会从参数服务器同步最新的Q网络模型参数。在这个闭环中有四个角色：采样器, 学习器, 参数服务器和重放缓冲区。

那么GORILA相比于DQN， 主要的**区别**在于：

- 对于采样器：GORILA里定义了一个捆绑模式（Bundled Mode），即采样器的策略与学习器中实时更新的Q-Network是捆绑的。

- 对于学习器： 学习器中对于Q-Network的参数梯度会发给参数服务器。

- 对于重放缓冲区：在GORILA里分两种形式，在本地模式下就存在采样器所在的机器上；而多机模式下将所有的数据聚合在分布式数据库中，这样的优点是可伸缩性好，缺点是会有额外的通信开销。

- 对于参数服务器：存储Q网络中参数的梯度（Gradient）的变化，好处是可以让Q网络进行回滚，并且可以通过多个梯度来使训练过程更加稳定。在分布式环境中，不可避免的就是稳定性问题（比如节点消失、网速变慢或机器变慢）。GORILA中采用了几个策略来解决这个问题，如丢弃过旧的和损失值（Loss）太过偏离均值时的梯度。

GORILA中可以配置多个学习器、采样器和参数服务器，放在多个进程或多台机器上以分布式的方式并行执行。如实验中参数服务器使用了31台机器，学习器和采样器进程都有100个。实验部分与DQN一样基于Atari平台。在使用相同参数的情况下，该框架中在49中的41个游戏中表现好于非并行版本传统DQN，同时训练耗时也有显著减少。

  <div align="center">
  <img src="./img/GORILA_arch.png" ch="500" />
  </div>
  <div align=center>图10.2.3 GORILA的架构 </div>

### A3C

**A3C**[<sup>[3]</sup>](#a3c)是一个基于Actor-Critic算法。在A3C里没有参数服务器、没有公用的重放缓冲区。

具体来说：

- 每一个工作器（worker）实际上包含一个采样器、一个学习器还有一个小的缓冲区（通常是先进先出）。

- 每一个工作器（worker）中学习器计算得出梯度后都发送给全局网络（global network）。每一个工作器（Worker）中采样器都可以用不同的探索策略与环境进行交互，这些样本可以存在一个小缓冲区中。

- 全局网络（global network）接收多组梯度后再更新参数，再把异步地把参数拷贝给所有工作器。

  <div align="center">
  <img src="./img/A3C_arch.png" ch="500" width="70%"/>
  </div>
  <div align=center>图10.2.4 A3C的架构 </div>

而A3C架构的**优点**是：

- 每一个采样器可以用不同的策略探索环境，使得样本更具有多样性，探索到的状态空间更大。

- 全局网络等所有工作器都传递了梯度后再更新，使训练更稳定。

- 大规模并行非常容易。

- 在A3C架构中，每个工作器都独自计算梯度，全局网络只负责使用梯度，所以全局网络的计算量并不大。在作者的原始实现中，A3C不需要GPU资源，只需要CPU即可在Atari等游戏上达到很好的效果。

但同时A3C本身存在着**问题**：

- 当模型变得复杂时，在CPU上计算梯度的耗时会变得非常大，而如果迁移到GPU上，由于每个工作器都需要一个模型的副本，又会需要大量的GPU资源。

- 当模型变大时，传输梯度和模型参数的网络开销也会变得巨大。

- 全局网络使用异步方式更新梯度，这意味着在训练过程中，部分梯度的方向并不正确，从而可能影响最终的训练效果。这个现象会随着工作器的数量增多变得越来越严重，这也一定程度上限制了A3C的横向扩展能力。

### ApeX

**ApeX**[<sup>[4]</sup>](#apex)是2018年在DQN, GORILA之后的又一个基于DQN的工作。
在ApeX的结构里，采样器可以有多个实例，而学习器只有一个。

它和DQN,GORILA的**差别**是：

- 对于采样器: 以不同的探索策略和环境交互。例如，有的采样器以更大的概率去探索，有的采样器以小概率去探索。

- 对于学习器: 和GORILA不同，ApeX里中心学习器只有一个，从重放缓冲区里拿到数据学习。

- 对于重放缓冲区：不是均匀采样，而是按照优先级来采样，从而让算法专注于那些重要的数据。

  <div align="center">
  <img src="./img/ApeX_arch.png" ch="500" width="80%"/>
  </div>
  <div align=center>图10.2.5 ApeX的架构 </div>

ApeX的架构上可以适配DQN（ApeX-DQN）或者DDPG（ApeX-DDPG）等算法。 

在ApeX[<sup>[4]</sup>](#apex)的实验里提供了ApeX架构在Atari环境上的测试。ApeX的一大优势是采样器可以很方便的扩展。 在ApeX[<sup>[4]</sup>](#apex)的实验里，ApeX DQN的采样器最大扩展到了360个CPU。采样器积以异步方式将采集到的经验发送给重放缓冲区。而学习器以异步方式拿数据。异步的交互方式解耦了采样器，学习器和重放缓冲区的联系。实验结果表示ApeX DQN和DQN，GORILA相比，在训练速度和效果上都更有优势。

### IMPALA

**IMPALA**[<sup>[5]</sup>](#impala)（Importance Weighted Actor-Learner Architectures）是基于Actor-Critic和A3C的改进，最大的创新是提出了V-trace算法，对off-policy现象做了一定的修正。

在IMPALA架构中，每个采样器都拥有一个模型的副本，采样器发送训练样本给学习器，学习器更新模型之后，会将新模型发送给采样器。在整个过程中，采样器和学习器以异步的方式运行，即学习器只要收到训练数据就会更新模型，不会等待所有的采样器；而采样器在学习器更新模型时依然在采样，不会等待最新的模型。

IMPALA与A3C具体的**区别**在于：

- 对于采样器来说：每一个采样器执行的行为策略不再是只来自一个学习器，可以来自多个学习器. 

- 对于重放缓冲区来说：IMPALA里有两种模式，一种是由先进先出的队列实现的重放缓冲区，本质上是一种on-policy的算法；另一种是由一个数据池实现，但是每次随机采样其中的数据；
显然这样的运行方式会产生行为策略和目标策略不一致的现象，即：训练用的样本不是由当前的目标策略产生，而是由行为策略产生的，这对算法的收敛提出新的挑战。在IMPALA中，作者在数学上推导出了一种严谨的修正方式：V-trace算法。该算法显著降低了（由和目标策略不一样的行为策略生成的）训练样本带来的影响。下表可知，使用V-trace修正之后，相比于其他几种修正方式，最终的收敛效果有明显提升。

  <div align="center">
  <img src="./img/IMPALA_arch.png" ch="500" width="60%"/>
  </div>
  <div align=center>图10.2.6 IMPALA的架构 </div>

### SEEDRL

IMPALA在神经网络模型比较简单的时候性能很好，但当神经网络变得复杂的时候，该架构也有瓶颈。主要的**问题**有以下几点：

- 采样的时候，推理（Inference）放在采样器上执行，因为采样器是运行在CPU上的，所以当神经网络变复杂之后，推理的耗时就会变得很长，影响最终的运行效率。

- 采样器上执行了两种操作，一个是和环境交互，另一个是用行为策略做推理。很多游戏或者环境都是单线程实现的，而神经网络的推理计算则可以使用多线程加速，将两种操作放在一起，整体上会降低CPU的使用率。

- 当模型很大的时候，模型参数的分发会占用大量的带宽。

因为**SEEDRL**[<sup>[6]</sup>](#seedrl)的工作就是解决这些问题，SEEDRL的架构对比见下图：

  <div align="center">
  <img src="./img/seed_rl.png" ch="500" width="90%"/>
  </div>
  <div align=center>图10.2.7 SEEDRL的架构 </div>

和IMPALA相比，SEEDRL的**区别**主要是：

- 把采样器上的推理过程和学习器放在一同一块TPU上。而采样器和学习器之间只交换状态和采取的动作。

在SEEDRL中，采样器和学习器分布在不同的节点中，采样器通过gRPC来和学习器进行通信，SEEDRL同样使用了V-Trace来进行off-policy修正。

## 小结与讨论

通过本章的学习，可以发现强化学习算法，尤其是分布式强化学习算法之间的架构是差距非常大的。体现在智能体的运行的硬件，交互的方式，智能体里不同模块之间的连接关系等都会有很大的差别。在下一章里，我们会讨论算法架构之间的差异会给设计强化学习系统带来什么样的挑战。

## 参考文献

<div id="dqn"></div>

1. Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning[J.. arXiv preprint arXiv:1312.5602, 2013.

<div id="gorila"></div>

2. Nair A, Srinivasan P, Blackwell S, et al. Massively parallel methods for deep reinforcement learning[J.. arXiv preprint arXiv:1507.04296, 2015.

<div id="a3c"></div>

3. Mnih V, Badia A P, Mirza M, et al. Asynchronous methods for deep reinforcement learning[C.//International conference on machine learning. PMLR, 2016: 1928-1937.

<div id="apex"></div>

4. D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt and D. Silver, "Distributed prioritized experience replay," arXiv preprint arXiv:1803.00933, 2018.

<div id="impala"></div>

5. Espeholt L, Soyer H, Munos R, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures[C.//International Conference on Machine Learning. PMLR, 2018: 1407-1416.

<div id="seedrl"></div>

6. Espeholt L, Marinier R, Stanczyk P, et al. Seed rl: Scalable and efficient deep-rl with accelerated central inference[J.. arXiv preprint arXiv:1910.06591, 2019.

<div id="alphastar"></div>

7. Arulkumaran K, Cully A, Togelius J. Alphastar: An evolutionary computation perspective[C.//Proceedings of the genetic and evolutionary computation conference companion. 2019: 314-315.

<div id="openfive"></div>

8. Berner C, Brockman G, Chan B, et al. Dota 2 with large scale deep reinforcement learning[J.. arXiv preprint arXiv:1912.06680, 2019.

<div id="moba"></div>

9. Ye D, Liu Z, Sun M, et al. Mastering complex control in moba games with deep reinforcement learning[C.//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(04): 6672-6679.
